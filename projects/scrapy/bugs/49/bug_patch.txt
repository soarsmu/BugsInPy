diff --git a/scrapy/core/engine.py b/scrapy/core/engine.py
index 40f19e4c..992327bf 100644
--- a/scrapy/core/engine.py
+++ b/scrapy/core/engine.py
@@ -16,7 +16,7 @@ from scrapy.exceptions import DontCloseSpider
 from scrapy.http import Response, Request
 from scrapy.utils.misc import load_object
 from scrapy.utils.reactor import CallLaterOnce
-from scrapy.utils.log import logformatter_adapter
+from scrapy.utils.log import logformatter_adapter, failure_to_exc_info
 
 logger = logging.getLogger(__name__)
 
@@ -135,13 +135,16 @@ class ExecutionEngine(object):
         d = self._download(request, spider)
         d.addBoth(self._handle_downloader_output, request, spider)
         d.addErrback(lambda f: logger.info('Error while handling downloader output',
-                                           extra={'spider': spider, 'failure': f}))
+                                           exc_info=failure_to_exc_info(f),
+                                           extra={'spider': spider}))
         d.addBoth(lambda _: slot.remove_request(request))
         d.addErrback(lambda f: logger.info('Error while removing request from slot',
-                                           extra={'spider': spider, 'failure': f}))
+                                           exc_info=failure_to_exc_info(f),
+                                           extra={'spider': spider}))
         d.addBoth(lambda _: slot.nextcall.schedule())
         d.addErrback(lambda f: logger.info('Error while scheduling new request',
-                                           extra={'spider': spider, 'failure': f}))
+                                           exc_info=failure_to_exc_info(f),
+                                           extra={'spider': spider}))
         return d
 
     def _handle_downloader_output(self, response, request, spider):
@@ -153,7 +156,8 @@ class ExecutionEngine(object):
         # response is a Response or Failure
         d = self.scraper.enqueue_scrape(response, request, spider)
         d.addErrback(lambda f: logger.error('Error while enqueuing downloader output',
-                                            extra={'spider': spider, 'failure': f}))
+                                            exc_info=failure_to_exc_info(f),
+                                            extra={'spider': spider}))
         return d
 
     def spider_is_idle(self, spider):
@@ -268,7 +272,11 @@ class ExecutionEngine(object):
 
         def log_failure(msg):
             def errback(failure):
-                logger.error(msg, extra={'spider': spider, 'failure': failure})
+                logger.error(
+                    msg,
+                    exc_info=failure_to_exc_info(failure),
+                    extra={'spider': spider}
+                )
             return errback
 
         dfd.addBoth(lambda _: self.downloader.close())
diff --git a/scrapy/core/scraper.py b/scrapy/core/scraper.py
index e5d8acea..244499be 100644
--- a/scrapy/core/scraper.py
+++ b/scrapy/core/scraper.py
@@ -10,7 +10,7 @@ from twisted.internet import defer
 from scrapy.utils.defer import defer_result, defer_succeed, parallel, iter_errback
 from scrapy.utils.spider import iterate_spider_output
 from scrapy.utils.misc import load_object
-from scrapy.utils.log import logformatter_adapter
+from scrapy.utils.log import logformatter_adapter, failure_to_exc_info
 from scrapy.exceptions import CloseSpider, DropItem, IgnoreRequest
 from scrapy import signals
 from scrapy.http import Request, Response
@@ -107,7 +107,8 @@ class Scraper(object):
         dfd.addErrback(
             lambda f: logger.error('Scraper bug processing %(request)s',
                                    {'request': request},
-                                   extra={'spider': spider, 'failure': f}))
+                                   exc_info=failure_to_exc_info(f),
+                                   extra={'spider': spider}))
         self._scrape_next(spider, slot)
         return dfd
 
@@ -153,7 +154,8 @@ class Scraper(object):
         logger.error(
             "Spider error processing %(request)s (referer: %(referer)s)",
             {'request': request, 'referer': referer},
-            extra={'spider': spider, 'failure': _failure}
+            exc_info=failure_to_exc_info(_failure),
+            extra={'spider': spider}
         )
         self.signals.send_catch_log(
             signal=signals.spider_error,
@@ -202,7 +204,8 @@ class Scraper(object):
             if download_failure.frames:
                 logger.error('Error downloading %(request)s',
                              {'request': request},
-                             extra={'spider': spider, 'failure': download_failure})
+                             exc_info=failure_to_exc_info(download_failure),
+                             extra={'spider': spider})
             else:
                 errmsg = download_failure.getErrorMessage()
                 if errmsg:
@@ -227,7 +230,8 @@ class Scraper(object):
                     spider=spider, exception=output.value)
             else:
                 logger.error('Error processing %(item)s', {'item': item},
-                             extra={'spider': spider, 'failure': output})
+                             exc_info=failure_to_exc_info(output),
+                             extra={'spider': spider})
         else:
             logkws = self.logformatter.scraped(output, response, spider)
             logger.log(*logformatter_adapter(logkws), extra={'spider': spider})
diff --git a/scrapy/downloadermiddlewares/robotstxt.py b/scrapy/downloadermiddlewares/robotstxt.py
index 9083482f..77e08b7e 100644
--- a/scrapy/downloadermiddlewares/robotstxt.py
+++ b/scrapy/downloadermiddlewares/robotstxt.py
@@ -11,6 +11,7 @@ from six.moves.urllib import robotparser
 from scrapy.exceptions import NotConfigured, IgnoreRequest
 from scrapy.http import Request
 from scrapy.utils.httpobj import urlparse_cached
+from scrapy.utils.log import failure_to_exc_info
 
 logger = logging.getLogger(__name__)
 
@@ -59,7 +60,8 @@ class RobotsTxtMiddleware(object):
         if failure.type is not IgnoreRequest:
             logger.error("Error downloading %(request)s: %(f_exception)s",
                          {'request': request, 'f_exception': failure.value},
-                         extra={'spider': spider, 'failure': failure})
+                         exc_info=failure_to_exc_info(failure),
+                         extra={'spider': spider})
 
     def _parse_robots(self, response):
         rp = robotparser.RobotFileParser(response.url)
diff --git a/scrapy/extensions/feedexport.py b/scrapy/extensions/feedexport.py
index 7c6849a7..3bc1c92c 100644
--- a/scrapy/extensions/feedexport.py
+++ b/scrapy/extensions/feedexport.py
@@ -22,6 +22,7 @@ from scrapy.utils.ftp import ftp_makedirs_cwd
 from scrapy.exceptions import NotConfigured
 from scrapy.utils.misc import load_object
 from scrapy.utils.python import get_func_args
+from scrapy.utils.log import failure_to_exc_info
 
 logger = logging.getLogger(__name__)
 
@@ -184,7 +185,8 @@ class FeedExporter(object):
         d.addCallback(lambda _: logger.info(logfmt % "Stored", log_args,
                                             extra={'spider': spider}))
         d.addErrback(lambda f: logger.error(logfmt % "Error storing", log_args,
-                                            extra={'spider': spider, 'failure': f}))
+                                            exc_info=failure_to_exc_info(f),
+                                            extra={'spider': spider}))
         return d
 
     def item_scraped(self, item, spider):
diff --git a/scrapy/log.py b/scrapy/log.py
index c3f9c422..5dabe569 100644
--- a/scrapy/log.py
+++ b/scrapy/log.py
@@ -1,51 +1,166 @@
-"""
-This module is kept to provide a helpful warning about its removal.
-"""
+# -*- coding: utf-8 -*-
 
+import sys
 import logging
 import warnings
+from logging.config import dictConfig
 
 from twisted.python.failure import Failure
+from twisted.python import log as twisted_log
 
+import scrapy
+from scrapy.settings import overridden_settings, Settings
 from scrapy.exceptions import ScrapyDeprecationWarning
 
 logger = logging.getLogger(__name__)
 
-warnings.warn("Module `scrapy.log` has been deprecated, Scrapy now relies on "
-              "the builtin Python library for logging. Read the updated "
-              "logging entry in the documentation to learn more.",
-              ScrapyDeprecationWarning, stacklevel=2)
 
+def failure_to_exc_info(failure):
+    """Extract exc_info from Failure instances"""
+    if isinstance(failure, Failure):
+        return (failure.type, failure.value, failure.tb)
 
-# Imports kept for backwards-compatibility
 
-DEBUG = logging.DEBUG
-INFO = logging.INFO
-WARNING = logging.WARNING
-ERROR = logging.ERROR
-CRITICAL = logging.CRITICAL
-SILENT = CRITICAL + 1
+class TopLevelFormatter(logging.Filter):
+    """Keep only top level loggers's name (direct children from root) from
+    records.
 
+    This filter will replace Scrapy loggers' names with 'scrapy'. This mimics
+    the old Scrapy log behaviour and helps shortening long names.
 
-def msg(message=None, _level=logging.INFO, **kw):
-    warnings.warn('log.msg has been deprecated, create a python logger and '
-                  'log through it instead',
-                  ScrapyDeprecationWarning, stacklevel=2)
+    Since it can't be set for just one logger (it won't propagate for its
+    children), it's going to be set in the root handler, with a parametrized
+    `loggers` list where it should act.
+    """
 
-    level = kw.pop('level', _level)
-    message = kw.pop('format', message)
-    # NOTE: logger.log doesn't handle well passing empty dictionaries with format
-    # arguments because of some weird use-case:
-    # https://hg.python.org/cpython/file/648dcafa7e5f/Lib/logging/__init__.py#l269
-    logger.log(level, message, *[kw] if kw else [])
+    def __init__(self, loggers=None):
+        self.loggers = loggers or []
 
+    def filter(self, record):
+        if any(record.name.startswith(l + '.') for l in self.loggers):
+            record.name = record.name.split('.', 1)[0]
+        return True
 
-def err(_stuff=None, _why=None, **kw):
-    warnings.warn('log.err has been deprecated, create a python logger and '
-                  'use its error method instead',
-                  ScrapyDeprecationWarning, stacklevel=2)
 
-    level = kw.pop('level', logging.ERROR)
-    failure = kw.pop('failure', _stuff) or Failure()
-    message = kw.pop('why', _why) or failure.value
-    logger.log(level, message, *[kw] if kw else [], extra={'failure': failure})
+DEFAULT_LOGGING = {
+    'version': 1,
+    'disable_existing_loggers': False,
+    'loggers': {
+        'scrapy': {
+            'level': 'DEBUG',
+        },
+        'twisted': {
+            'level': 'ERROR',
+        },
+    }
+}
+
+
+def configure_logging(settings=None):
+    """Initialize and configure default loggers
+
+    This function does:
+      - Route warnings and twisted logging through Python standard logging
+      - Set FailureFormatter filter on Scrapy logger
+      - Assign DEBUG and ERROR level to Scrapy and Twisted loggers respectively
+      - Create a handler for the root logger according to given settings
+    """
+    if not sys.warnoptions:
+        # Route warnings through python logging
+        logging.captureWarnings(True)
+
+    observer = twisted_log.PythonLoggingObserver('twisted')
+    observer.start()
+
+    dictConfig(DEFAULT_LOGGING)
+
+    if isinstance(settings, dict):
+        settings = Settings(settings)
+
+    if settings:
+        logging.root.setLevel(logging.NOTSET)
+
+        if settings.getbool('LOG_STDOUT'):
+            sys.stdout = StreamLogger(logging.getLogger('stdout'))
+
+        # Set up the default log handler
+        filename = settings.get('LOG_FILE')
+        if filename:
+            encoding = settings.get('LOG_ENCODING')
+            handler = logging.FileHandler(filename, encoding=encoding)
+        elif settings.getbool('LOG_ENABLED'):
+            handler = logging.StreamHandler()
+        else:
+            handler = logging.NullHandler()
+
+        formatter = logging.Formatter(
+            fmt=settings.get('LOG_FORMAT'),
+            datefmt=settings.get('LOG_DATEFORMAT')
+        )
+        handler.setFormatter(formatter)
+        handler.setLevel(settings.get('LOG_LEVEL'))
+        handler.addFilter(TopLevelFormatter(['scrapy']))
+        logging.root.addHandler(handler)
+
+
+def log_scrapy_info(settings):
+    logger.info("Scrapy %(version)s started (bot: %(bot)s)",
+                {'version': scrapy.__version__, 'bot': settings['BOT_NAME']})
+
+    logger.info("Optional features available: %(features)s",
+                {'features': ", ".join(scrapy.optional_features)})
+
+    d = dict(overridden_settings(settings))
+    logger.info("Overridden settings: %(settings)r", {'settings': d})
+
+
+class StreamLogger(object):
+    """Fake file-like stream object that redirects writes to a logger instance
+
+    Taken from:
+        http://www.electricmonk.nl/log/2011/08/14/redirect-stdout-and-stderr-to-a-logger-in-python/
+    """
+    def __init__(self, logger, log_level=logging.INFO):
+        self.logger = logger
+        self.log_level = log_level
+        self.linebuf = ''
+
+    def write(self, buf):
+        for line in buf.rstrip().splitlines():
+            self.logger.log(self.log_level, line.rstrip())
+
+
+class LogCounterHandler(logging.Handler):
+    """Record log levels count into a crawler stats"""
+
+    def __init__(self, crawler, *args, **kwargs):
+        super(LogCounterHandler, self).__init__(*args, **kwargs)
+        self.crawler = crawler
+
+    def emit(self, record):
+        sname = 'log_count/{}'.format(record.levelname)
+        self.crawler.stats.inc_value(sname)
+
+
+def logformatter_adapter(logkws):
+    """
+    Helper that takes the dictionary output from the methods in LogFormatter
+    and adapts it into a tuple of positional arguments for logger.log calls,
+    handling backward compatibility as well.
+    """
+    if not {'level', 'msg', 'args'} <= set(logkws):
+        warnings.warn('Missing keys in LogFormatter method',
+                      ScrapyDeprecationWarning)
+
+    if 'format' in logkws:
+        warnings.warn('`format` key in LogFormatter methods has been '
+                      'deprecated, use `msg` instead',
+                      ScrapyDeprecationWarning)
+
+    level = logkws.get('level', logging.INFO)
+    message = logkws.get('format', logkws.get('msg'))
+    # NOTE: This also handles 'args' being an empty dict, that case doesn't
+    # play well in logger.log calls
+    args = logkws if not logkws.get('args') else logkws['args']
+
+    return (level, message, args)
diff --git a/scrapy/pipelines/files.py b/scrapy/pipelines/files.py
index c0192b86..250f46ad 100644
--- a/scrapy/pipelines/files.py
+++ b/scrapy/pipelines/files.py
@@ -25,6 +25,7 @@ from scrapy.pipelines.media import MediaPipeline
 from scrapy.exceptions import NotConfigured, IgnoreRequest
 from scrapy.http import Request
 from scrapy.utils.misc import md5sum
+from scrapy.utils.log import failure_to_exc_info
 
 logger = logging.getLogger(__name__)
 
@@ -212,7 +213,8 @@ class FilesPipeline(MediaPipeline):
         dfd.addErrback(
             lambda f:
             logger.error(self.__class__.__name__ + '.store.stat_file',
-                         extra={'spider': info.spider, 'failure': f})
+                         exc_info=failure_to_exc_info(f),
+                         extra={'spider': info.spider})
         )
         return dfd
 
diff --git a/scrapy/pipelines/media.py b/scrapy/pipelines/media.py
index 55ef05ad..21b8b898 100644
--- a/scrapy/pipelines/media.py
+++ b/scrapy/pipelines/media.py
@@ -8,6 +8,7 @@ from twisted.python.failure import Failure
 from scrapy.utils.defer import mustbe_deferred, defer_result
 from scrapy.utils.request import request_fingerprint
 from scrapy.utils.misc import arg_to_iter
+from scrapy.utils.log import failure_to_exc_info
 
 logger = logging.getLogger(__name__)
 
@@ -70,7 +71,7 @@ class MediaPipeline(object):
         dfd.addCallback(self._check_media_to_download, request, info)
         dfd.addBoth(self._cache_result_and_execute_waiters, fp, info)
         dfd.addErrback(lambda f: logger.error(
-            f.value, extra={'spider': info.spider, 'failure': f})
+            f.value, exc_info=failure_to_exc_info(f), extra={'spider': info.spider})
         )
         return dfd.addBoth(lambda _: wad)  # it must return wad at last
 
@@ -127,6 +128,7 @@ class MediaPipeline(object):
                     logger.error(
                         '%(class)s found errors processing %(item)s',
                         {'class': self.__class__.__name__, 'item': item},
-                        extra={'spider': info.spider, 'failure': value}
+                        exc_info=failure_to_exc_info(value),
+                        extra={'spider': info.spider}
                     )
         return item
diff --git a/scrapy/utils/signal.py b/scrapy/utils/signal.py
index d4cc4130..d9a59e16 100644
--- a/scrapy/utils/signal.py
+++ b/scrapy/utils/signal.py
@@ -8,6 +8,7 @@ from twisted.python.failure import Failure
 from scrapy.xlib.pydispatch.dispatcher import Any, Anonymous, liveReceivers, \
     getAllReceivers, disconnect
 from scrapy.xlib.pydispatch.robustapply import robustApply
+from scrapy.utils.log import failure_to_exc_info
 
 logger = logging.getLogger(__name__)
 
@@ -47,7 +48,8 @@ def send_catch_log_deferred(signal=Any, sender=Anonymous, *arguments, **named):
         if dont_log is None or not isinstance(failure.value, dont_log):
             logger.error("Error caught on signal handler: %(receiver)s",
                          {'receiver': recv},
-                         extra={'spider': spider, 'failure': failure})
+                         exc_info=failure_to_exc_info(failure),
+                         extra={'spider': spider})
         return failure
 
     dont_log = named.pop('dont_log', None)
